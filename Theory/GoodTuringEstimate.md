Good-Turing估计的基本思想：从概率的总量中分配很小比例给未见事件。这样就需要将所有观察到事件的概率调小。调小的方法是**越不可信的统计折扣越多**。

例如，统计词典中词的概率，假设语料库中：

+ 出现r次的词有$$N_r$$个
+ 未出现的词数量为$$N_0$$
+ 语料中总的词数为$$N=\Sigma\limits_{r=1}^{\infty}{rN_r}$$
+ 出现r次的词在整个语料库上的相对频度为$$\frac{r}{N}$$，如果不做任何处理，概率估计==相对频度

当$$r$$比较小时，它的估计可能不可靠，因此在计算出现$$r$$次的词的概率时，要使用一个更小的系数$$d_r$$(而不是直接使用$$r$$)。

Good-Turing Estimate按照下面的公式计算:
$$d_r=(r+1)\frac{N_{r+1}}{N_{r}}$$
显然有$$\Sigma\limit_{r}{d_rN_r}=N$$

由于词的频率分布服从Zipf定律，即出现一次的词的数量比出现两次的多，出现两次的比三次的多。 可以看出$$r$$越大，词的数量$$N_r$$越小，即$$N_{r+1}<N_{r}$$。 因此，一般情况下$$d_r<r$$，而$$d_0>0$$。 这样就给为出现的词赋予了一个很小的非零值，从而解决了零概率的问题。同时下调了出现概率很低的词的概率。

但是，在实际场景中，可能会遇到如下几个问题：

+ 首先，在自然语言处理中，一般会设置一个阈值$$T$$，仅对出现次数小于$$T$$的词做上述调整。（这种情况下只需对$$r$$的定义重新定义即可）
+ 实际统计中$$N_{r+1}<N_{r}$$不一定成立(频次高的词数小于频次低的词数)
12. $$N_r=0$$情况也可能出现（在某些统计频次下没有词）

因此在实际中：

1. 需要使用曲线拟合的方式替换掉原有的$$N_r$$
2. 使用如下Kartz退避公式计算$$d_r$$:


$$d_r=\frac{(r+1)\frac{N_{r+1}}{N_r}-r\frac{(k+1)N_{k+1}}{N_1}}{1-\frac{(k+1)N_{k+1}}{N_1}}, 1 \le r \le k$$





